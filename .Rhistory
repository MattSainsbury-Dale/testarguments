coverage = mean((Z > Z_percentile_5) & (Z < Z_percentile_95))))
}
diagnostics <- test_arguments(fun, df_train, df_test, diagnostic_fun,
arguments = list(link = c("log", "square-root"),
nres = 1:3))
## Visualise the performance
plot_diagnostics(diagnostics, c("nres", "link"))
devtools::load_all(".")
devtools::document()
devtools::load_all(".")
devtools::document()
devtools::load_all(".")
devtools::document()
## Visualise the performance
plot_diagnostics(diagnostics, c("nres", "link"))
diagnostics <- test_arguments(fun, df_train, df_test, diagnostic_fun,
arguments = list(link = c("log", "square-root"),
nres = 1:3))
## Visualise the performance
plot_diagnostics(diagnostics, c("nres", "link"))
plot_diagnostics(diagnostics, c("nres", "link"), focused_args = "nres")
plot_diagnostics(diagnostics, c("nres", "link"), focused_args = "log")
plot_diagnostics(diagnostics, c("nres", "link"), focused_args = "link")
devtools::load_all(".")
devtools::document
devtools::document()
devtools::load_all(".")
devtools::document()
devtools::load_all(".")
devtools::load_all(".")
?test_arguments
## Load FRK package and create training and testing data
library("testarguments")
library("FRK")
library("sp")
data("Poisson_simulated")
n <- nrow(Poisson_simulated)
train_id <- sample(1:n, round(n/2))
df_train <- Poisson_simulated[train_id, ]
df_test  <- Poisson_simulated[-train_id, ]
## Define the function which creates predictions.
## In this example, we wish to test values of the arguments link and nres.
fun <- function(df_train, df_test, link, nres) {
## Convert dataframes to Spatial* objects (as required by FRK)
coordinates(df_train) <- ~ x + y
coordinates(df_test) <- ~ x + y
BAUs <- auto_BAUs(manifold = plane(), data = rbind(df_train, df_test))
## Fit using df_train, predict at df_test locations
S <- FRK(f = Z ~ 1, data = list(df_train), BAUs = BAUs, response = "poisson",
link = link, nres = nres)
pred <- predict(S, newdata = df_test, type = "response")
## NB: returned object needs to be a matrix or data.frame with named columns
return(pred$newdata@data)
}
## diagnostic_fun should return a named vector
diagnostic_fun <- function(df_test) {
with(df_test,
c(RMSPE = sqrt(mean((p_Z - Z)^2)),
coverage = mean((Z > Z_percentile_5) & (Z < Z_percentile_95))))
}
diagnostics <- test_arguments(fun, df_train, df_test, diagnostic_fun,
arguments = list(link = c("log", "square-root"),
nres = 1:3))
## Visualise the performance
plot_diagnostics(diagnostics, c("nres", "link"))
plot_diagnostics(diagnostics, c("nres", "link"), focused_args = "nres")
devtools::load_all(".")
library("FRK")
library("sp")
library("pROC") # AUC score
n <- 5000                                                  # sample size
RNGversion("3.6.0"); set.seed(1)
data("MODIS_cloud_df") # MODIS dataframe stored in FRK (FRKTMB branch)
train_id <- sample(1:nrow(MODIS_cloud_df), n, replace = FALSE)
df_train <- MODIS_cloud_df[train_id, ]                     # training set
df_test  <- MODIS_cloud_df[-train_id, ]                    # testing set
fun <- function(df_train, df_test, link, nres) {
## Convert dataframes to Spatial* objects (as required by FRK)
coordinates(df_train) <- ~ x + y
coordinates(df_test) <- ~ x + y
## BAUs (just use a grid over the spatial domain of interest)
BAUs    <- SpatialPixelsDataFrame(points = expand.grid(x = 1:225, y = 1:150),
data = expand.grid(x = 1:225, y = 1:150))
## Fit using df_train
df_train$k_Z <- 1 # size parameter of the binomial distribution
S <- FRK(f = z ~ 1, data = list(df_train), BAUs = BAUs, response = "binomial",
link = link, nres = nres)
## Predict using df_test
pred <- predict(S, newdata = df_test, type = "response")
## Returned object must be a matrix-like object with named columns
return(pred$newdata@data)
}
diagnostic_fun <- function(df) {
with(df, c(
Brier = mean((z - p_Z)^2),
AUC = as.numeric(pROC::auc(z, p_Z))
))
}
diagnostic_fun <- function(df) {
with(df, c(
Brier = mean((z - p_Z)^2),
AUC = as.numeric(pROC::auc(z, p_Z))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(link = c("logit", "probit"), nres = 1:3)
)
plot_diagnostics(testargs_object)
plot_diagnostics(testargs_object, focused_args = "nres")
optimal_arguments(testargs_object)
testargs_object <- readRDS("~/Dropbox/testarguments/Heaton_testargs_object.rds")
optimality_criterion <- list(RMSE = which.min, MAE = which.min, CRPS = which.min,
Cov95 = function(x) which.min(abs(x - 0.95)),
int_score = which.min, Time = which.min)
optimal_arguments(testargs_object, optimality_criterion)
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(link = c("logit", "probit"), nres = 1:3)
)
optimal_arguments(testargs_object)
optimality_criterion <- list(Brier = which.min, AUC = which.max, Time = which.min)
Objects of class `testargs` can be combined using `bind()`. For computing the optimal arguments from a `testargs` object, see `optimal_arguments()`. The optimality criterion is diagnsotics dependent (e.g., we typically wish to *minimise* the Brier score and run time, but *maximise* the AUC score). For this reason, `optimal_arguments()` allows one to set the optimality criterion for each rule individually.
```r
optimality_criterion <- list(Brier = which.min, AUC = which.max, Time = which.min)
optimal_arguments(testargs_object, optimality_criterion)
```
More complicated criteria are possible: For instance, if one of the diagnostics is Cov90 (the coverage from 90% prediction intervals), then one would use something like `list(Cov90 = function(x) which.min(abs(x - 0.90)))`.
optimal_arguments(testargs_object, optimality_criterion)
optimality_criterion <- list(Brier = which.min, AUC = which.max, Time = which.min)
optimal_arguments(testargs_object, optimality_criterion)
xtable::xtable(optimal_arguments(testargs_object, optimality_criterion))
?xtable::xtable
xtable::xtable(optimal_arguments(testargs_object, optimality_criterion), type = "HTML")
xtable::xtable(optimal_arguments(testargs_object, optimality_criterion), type = "html")
print(xtable::xtable(optimal_arguments(testargs_object, optimality_criterion)), type = "html")
list(min)
lapply(1:3, min)
devtools::load_all(".")
optimal_arguments(testargs_object, optimality_criterion)
optimal_arguments(testargs_object)
testargs_object <- readRDS("~/Dropbox/testarguments/Heaton_testargs_object.rds")
devtools::load_all(".")
plot_diagnostics(testargs_object) + scale_x_continuous(trans = "log10")
x <- seq(0, 1, length.out = 100)
x <- seq(0, 1, length.out = 100)
mu <- 5 + 0.3 * x + 0.4 * x^2 + 0.7 * x^3
Z <- mu + rnorm(n, sd = 0.1)
df_train <- data.frame(x = x, Z = Z)
n <- 100                                          # data set size
x <- seq(0, 1, length.out = n)                    # covariate
mu <- 1 + x + x^2 + x^3                           # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
df <- data.frame(x = x, Z = Z)
train_id <- sample(1:n, n/2, replace = FALSE)
df_train <- df[train_id, ]                        # training set
df_test  <- df[-train_id, ]                       # testing set
plot(x, Z)
n <- 100                                          # data set size
x <- seq(0, 5, length.out = n)                    # covariate
mu <- 1 + x + x^2 + x^3                           # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(0, 1, length.out = n)                    # covariate
mu <- 1 + x + x^2 + x^3                           # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                    # covariate
mu <- 1 + x + x^2 + x^3 - x^4                           # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3                           # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + -x^2 + x^3                           # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 + x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 + -x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.1)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 + -x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + 3 * x^3 + -x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + 3 * x^3 - 4 * x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 1, length.out = n)                   # covariate
mu <- 1 + x + x^2 + 3 * x^3 - 2 * x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
mu <- 1 + x + 2 * x^2 + 3 * x^3 - 2 * x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
mu <- 1 + x + 2 * x^2 + 3 * x^3 - 3 * x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
mu <- 1 + x + 2 * x^2 + 3 * x^3 - 3 * x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
mu <- 1 + x + x^2 + x^3 + x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
x <- seq(-2, 2, length.out = n)                   # covariate
mu <- 1 + x + 2 * x^2 + 3 * x^3 - 3 * x^4                         # true mean
mu <- 1 + x + x^2 + x^3 + x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
mu <- 1 + x + x^2 + x^3 - x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
n <- 100                                          # data set size
x <- seq(-1, 2, length.out = n)                   # covariate
mu <- 1 + x + 2 * x^2 + 3 * x^3 - 3 * x^4                         # true mean
mu <- 1 + x + x^2 + x^3 - x^4                         # true mean
Z <- mu + rnorm(n, sd = 0.05)                      # data
plot(x, Z)
M <- lm(Z ~ poly(x,4), data = df_train)
predict.lm(newdata = df_test)
predict.lm(M, newdata = df_test)
Z_hat <- predict.lm(M, newdata = df_test)
data.frame(Z_hat = Z_hat)
diagnostic_fun <- function(df) {
with(df, c(
RMSE = mean((Z - Z_hat)^2)
))
}
Z_hat <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
Z_hat
Z_hat %>% head
head(Z_hat)
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
class(pred)
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2)),
coverage = mean((Z < upr) & (Z > lwr))
))
}
n <- 100                                          # data set size
x <- seq(-1, 2, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 - x^4                     # true mean
Z <- mu + rnorm(n, sd = 0.05)                     # data
plot(x, Z)
df <- data.frame(x = x, Z = Z)
train_id <- sample(1:n, n/2, replace = FALSE)
df_train <- df[train_id, ]                        # training set
df_test  <- df[-train_id, ]                       # testing set
fun <- function(df_train, df_test, degree) {
## Fit a polygo
M <- lm(Z ~ poly(x, degree), data = df_train)
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
## NB: Predictions must be a matrix-like object with named columns.
## In this example, pred is a matrix with columns fit (the predicted-values),
## and lwr and upr (the lower and upper bounds of the 90% confidence interval)
return(pred)
}
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2)),
coverage = mean((Z < upr) & (Z > lwr)),
interval_width = mean(upr - lwr)
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
devtools::load_all(".")
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
pred
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
is.null(names(pred))
head(pred)
names(pred) <- c("a", "b", "c")
head(pred)
is.null(names(pred))
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
head(pred)
is.null(names(pred))
names(pred) <- c("a", "b", "c")
head(pred)
names(pred) <- c("a", "b", "c")
head(pred)
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
pred[1, ]
pred[2, ]
slots(pred)
slotNames(pred)
slotNames("matrix")
?`matrix-class`
dimnames(pred)
dimnames(pred)[[2]]
dimnames(data.frame(x = 5))
dimanmes(matrix(1))
dimnames(matrix(1))
names(pred) <- dimnames(pred)[[2]]
devtools::load_all(".")
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
n <- 10000                                        # data set size
x <- seq(-1, 2, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 - x^4                     # true mean
Z <- mu + rnorm(n, sd = 0.05)                     # data
plot(x, Z)
df <- data.frame(x = x, Z = Z)
train_id <- sample(1:n, n/2, replace = FALSE)
df_train <- df[train_id, ]                        # training set
df_test  <- df[-train_id, ]                       # testing set
fun <- function(df_train, df_test, degree) {
## Fit a polygo
M <- lm(Z ~ poly(x, degree), data = df_train)
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
## NB: Predictions must be a matrix-like object with named columns.
## In this example, pred is a matrix with columns fit (the predicted-values),
## and lwr and upr (the lower and upper bounds of the 90% confidence interval)
return(pred)
}
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
n <- 100000                                       # data set size
x <- seq(-1, 2, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 - x^4                     # true mean
Z <- mu + rnorm(n, sd = 0.05)                     # data
df <- data.frame(x = x, Z = Z)
train_id <- sample(1:n, n/2, replace = FALSE)
df_train <- df[train_id, ]                        # training set
df_test  <- df[-train_id, ]                       # testing set
fun <- function(df_train, df_test, degree) {
## Fit a polygo
M <- lm(Z ~ poly(x, degree), data = df_train)
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
## NB: Predictions must be a matrix-like object with named columns.
## In this example, pred is a matrix with columns fit (the predicted-values),
## and lwr and upr (the lower and upper bounds of the 90% confidence interval)
return(pred)
}
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
n <- 100                                          # data set size
x <- seq(-1, 2, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 - x^4                     # true mean
Z <- mu + rnorm(n, sd = 0.05)                     # data
df <- data.frame(x = x, Z = Z)
train_id <- sample(1:n, n/2, replace = FALSE)
df_train <- df[train_id, ]                        # training set
df_test  <- df[-train_id, ]                       # testing set
fun <- function(df_train, df_test, degree) {
## Fit a polygo
M <- lm(Z ~ poly(x, degree), data = df_train)
pred <- predict.lm(M, newdata = df_test, interval = "confidence", level = 0.99)
## NB: Predictions must be a matrix-like object with named columns.
## In this example, pred is a matrix with columns fit (the predicted-values),
## and lwr and upr (the lower and upper bounds of the 90% confidence interval)
return(pred)
}
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2)),
MAE  = mean(abs(Z - fit))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 0:4)
)
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(degree = 1:4)
)
plot_diagnostics(testargs_object)
pred <- predict.lm(M, newdata = df_test)      # predict over the test set
pred
class(pred)
## NB: Predictions must be a matrix-like object with named columns.
## In this example, pred is a matrix with columns fit (the predicted-values)
names(pred) <- "fit"
pred
pred <- predict.lm(M, newdata = df_test)      # predict over the test set
pred_values <- predict.lm(M, newdata = df_test) # Predict over the test set
as.matrix(pred_values)
n <- 100                                          # data set size
x <- seq(-1, 2, length.out = n)                   # covariate
mu <- 1 + x + x^2 + x^3 - x^4                     # true mean
Z <- mu + rnorm(n, sd = 0.05)                     # data
df <- data.frame(x = x, Z = Z)
train_id <- sample(1:n, n/2, replace = FALSE)
df_train <- df[train_id, ]                        # training set
df_test  <- df[-train_id, ]                       # testing set
fun <- function(df_train, df_test, d) {
## Fit a polynomial model of degree d, and predict over the test set
M <- lm(Z ~ poly(x, d), data = df_train)
pred <- predict.lm(M, newdata = df_test)
## NB: Predictions must be a matrix-like object with named columns
pred <- as.matrix(pred)
names(pred) <- "fit"
return(pred)
}
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2)),
MAE  = mean(abs(Z - fit))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(d = 1:4)
)
d = 1
## Fit a polynomial model of degree d, and predict over the test set
M <- lm(Z ~ poly(x, d), data = df_train)
pred <- predict.lm(M, newdata = df_test)
## NB: Predictions must be a matrix-like object with named columns
pred <- as.matrix(pred)
names(pred) <- "fit"
head(pred)
colnames(pred) <- "fit"
head(pred)
fun <- function(df_train, df_test, d) {
## Fit a polynomial model of degree d, and predict over the test set
M <- lm(Z ~ poly(x, d), data = df_train)
pred <- predict.lm(M, newdata = df_test)
## NB: Predictions must be a matrix-like object with named columns
pred <- as.matrix(pred)
colnames(pred) <- "fit"
return(pred)
}
diagnostic_fun <- function(df) {
with(df, c(
RMSE = sqrt(mean((Z - fit)^2)),
MAE  = mean(abs(Z - fit))
))
}
testargs_object <- test_arguments(
fun, df_train, df_test, diagnostic_fun,
arguments = list(d = 1:4)
)
plot_diagnostics(testargs_object)
optimal_arguments(testargs_object)
?lm
